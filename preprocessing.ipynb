{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(path = 'YandexRelPredChallenge.txt'):\n",
    "    yandex_log = []\n",
    "    with open(path, 'r') as f:\n",
    "        yandex_log = f.readlines()\n",
    "\n",
    "    yandex_log_ = []\n",
    "    for line in yandex_log:\n",
    "        line = line.strip()\n",
    "        yandex_log_.append(line.split('\\t'))\n",
    "        \n",
    "    return yandex_log_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by Thanos Efthymiou\n",
    "\n",
    "def queries_pickle(file_lines):    \n",
    "    \n",
    "    queries= {}\n",
    "    last_session = -1\n",
    "    \n",
    "    for line in file_lines:\n",
    "        \n",
    "        if line[0] != last_session:\n",
    "            last_session = line[0]\n",
    "            session_queries = []\n",
    "        \n",
    "        \n",
    "        if line[2] =='Q':\n",
    "            \n",
    "            session_queries.append(line[3])\n",
    "            \n",
    "            if line[3] not in queries:\n",
    "                \n",
    "                queries[line[3]] = []\n",
    "                queries[line[3]].append([line[5:],[]])\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                queries[line[3]].append([line[5:],[]])\n",
    "                \n",
    "            \n",
    "        else:\n",
    "    \n",
    "            \n",
    "            if line[3] in queries[session_queries[-1]][-1][0]:\n",
    "                \n",
    "                queries[session_queries[-1]][-1][1].append(line[3])\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                for sess_q in reversed(session_queries):\n",
    "                    if line[3] in queries[sess_q][-1][0]:\n",
    "                        queries[sess_q][-1][1].append(line[3])\n",
    "                        break\n",
    "                    \n",
    "    return queries\n",
    "        \n",
    "        \n",
    "        \n",
    "def queries_clicks(queries):\n",
    "    \n",
    "    \n",
    "    query_doc_clicks = {}\n",
    "\n",
    "\n",
    "    for query in queries.keys():\n",
    "        query_doc_clicks[query] = []\n",
    "        for docs in queries[query]:\n",
    "            \n",
    "            doc_clicks = [0]*10\n",
    "            for click in docs[1]:\n",
    "            \n",
    "                doc_clicks[docs[0].index(click)] = 1\n",
    "            \n",
    "            query_doc_clicks[query].append(doc_clicks)\n",
    "   \n",
    "    return query_doc_clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data (use dataset from IR1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/media/georgios/LENOVO/gnsid/Documents/MSc/IR2/YandexRelPredChallenge.txt'\n",
    "\n",
    "lines = read_file(path)\n",
    "\n",
    "# use both variables as global\n",
    "queries_dict = queries_pickle(lines)\n",
    "query_doc_clicks = queries_clicks(queries_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2536', '2506']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506', '1606', '2524']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506', '1606']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506', '2510', '2536']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506', '1606']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506', '1606', '1606']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], []]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506', '1606', '2536']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['1606']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['1606']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506', '1606', '2536']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506', '1606', '2536']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], ['2506']]\n",
      "[['2506', '1606', '2536', '2512', '2524', '2510', '2513', '2509', '2526', '2519'], []]\n"
     ]
    }
   ],
   "source": [
    "for q in queries_dict['275']:\n",
    "    print (q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for c in query_doc_clicks['275']:\n",
    "    print (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Document Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "# use it as global variable\n",
    "# all possible click combinations of the 10 documents in a SERP\n",
    "lst = list(itertools.product([0, 1], repeat=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serp2d(serp_clicks, d_pos):\n",
    "    \"\"\"\n",
    "    serp_clicks: serp list with zeros and ones if the doc at the respective position was clicked,\n",
    "    d_pos: the position of the document for which we are making the representation\n",
    "\n",
    "    Output: representation of document d with a vector of size of 10240.\n",
    "    \"\"\"\n",
    "\n",
    "    # init representation\n",
    "    d = [0]*10240\n",
    "\n",
    "    clicks = tuple(serp_clicks)\n",
    "\n",
    "    index = lst.index(clicks) \n",
    "    pos = index + d_pos * 1024\n",
    "\n",
    "    d[pos] = 1\n",
    "\n",
    "    return np.array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_repres(query_id, doc_id):\n",
    "    doc_repr = np.zeros(10240)\n",
    "    if query_id in queries_dict.keys():\n",
    "\n",
    "        for index, serp_and_clicked_docs in enumerate(queries_dict[query_id]):\n",
    "            serp = serp_and_clicked_docs[0]\n",
    "            if doc_id in serp:\n",
    "                serp_clicks = query_doc_clicks[query_id][index]\n",
    "                doc_pos = serp.index(doc_id)\n",
    "                #print(serp2d(serp_clicks, doc_pos).index(1))\n",
    "                doc_repr += serp2d(serp_clicks, doc_pos)\n",
    "    \n",
    "    return doc_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = '275'\n",
    "doc_id = '2506'\n",
    "d_r = get_doc_repres(query_id, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_r[512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
